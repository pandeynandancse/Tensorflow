example:  weather forecast, stock price etc.

Univariate time series : at each time one  value is changing

multivaiate : at each time multiple value is changing eg. birth rate and death rate at each timestamp


value changing with respect to time


imputed data/value   ::: filling missing values/data


time series also used in word recognition from sound wave eg, speech to text generation.





															Common patterns in time series :
trend  ---- The increasing or decreasing value in the series.  --->> may be upward or downward

seasonality   ---  seasonality is the presence of variations that occur at specific regular intervals less than a year, such as weekly, monthly, or quarterly
            -----  The repeating short-term cycle in the series.

occasional spike   ----- depends on some occassion --- eg. most tweets are done on twitter  when some great occasion is there as republic day, gandhi jayanti, ram navammi etc..


forecast learned patterns -->> experience gained from past patterns via graph is applied for future


noise  --- unpredictable change in time series data



autocorrelation ---- data that follows predictable shape ,even if scale is different




A stationary (time) series is one whose statistical properties such as the mean, variance and autocorrelation are all constant over time. Hence, a non-stationary series is one whose statistical properties change over time.





														Metrics

error = forecast - actual
mean squared error
root mean squared error
mean absolute error or mean absolute deviation(MAE)
mean absolute percentage error


naive forecast in tensorflow:
tf.keras.metrics.mean_absolute_error(x_valid,naive_forecast).numpy()







						
													Approaches 



Moving Average vs differencing


a) trailling window (TW) moving average of differenced series
b) centered window (CW) moving average of past series 

CW can smooth past values not present values
CW can be more accurate than TW



dataset.window(n, shift=1,drop_remainder = True)    //// drop remaider gives list with only of size n

data series/list to window feature ad labels -->> window_shift, flat_map,map,shuffle,batch



Sequence bias is when the order of things can impact the selection of things. For example, if I were to ask you your favorite TV show, and listed "Game of Thrones", "Killing Eve", "Travellers" and "Doctor Who" in that order, you're probably more likely to select 'Game of Thrones' as you are familiar with it, and it's the first thing you see. Even if it is equal to the other TV shows. So, when training data in a dataset, we don't want the sequence to impact the training in a similar way, so it's good to shuffle them up. 






What’s the correct line of code to split an n column window into n-1 columns for features and 1 column for a label??






If time values are in time[], series values are in series[] and we want to split the series into training and validation at time 1000, what is the correct code?
 time_train = time[:split_time]
x_train = series[:split_time]
time_valid = time[split_time:]
x_valid = series[split_time:]





If you want to inspect the learned parameters in a layer after training, what’s a good technique to use?
Assign a variable to the layer and add it to the model using that variable. Inspect its properties after training





How to set learning rate --->>> lr




If you want to amend the learning rate of the optimizer on the fly, after each epoch, what do you do?
Use a LearningRateScheduler object in the callbacks namespace and assign that to the callback




What is a windowed dataset?
A fixed-size subset of a time series








															Time series with RNN


Input shape = (batch_size, time_stamps, dims)     //// for univariate   ---->> input in memory cells is of dimensions (4*1)  --->>> if memory cell is compromised of three neurons then size of output willl be of 4*3 and 30 i.e. timeststamps means no of memory cells i.e. 4*3 matrix 30 times


SimpleRNN(retrun sequences =True)  --->> each ouptut is fed into next layer as input.


HUber loss function -->> less sensitive to outliers




What does a Lambda layer in a neural network do?
allows you to execute arbitrary code while training 




What does the axis parameter of tf.expand_dims do?
Defines the dimension index at which you will expand the shape of the tensor





If you want to clear out all temporary variables that tensorflow might have from previous sessions, what code do you run?
tf.keras.backend.clear_session() 




If your CSV file has a header that you don’t want to read into your dataset, what do you execute before iterating through the file using a ‘reader’ object?
next(reader)





When you read a row from a reader and want to cast column 2 to another data type, for example, a float, what’s the correct syntax?
float(row[2])

